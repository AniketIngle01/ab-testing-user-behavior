# Advanced A/B Testing Analysis Project
# Professional implementation with statistical rigor and business insights

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu
import warnings
warnings.filterwarnings('ignore')
import os
from datetime import datetime

# Set style for professional plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class ABTestAnalyzer:
    """
    Professional A/B Testing Analysis Framework
    Implements statistical rigor and business intelligence best practices
    """
    
    def __init__(self, data_path="data/ab_test_web_data.csv"):
        """Initialize the analyzer with data loading and validation"""
        self.df = None
        self.results = {}
        self.load_and_validate_data(data_path)
        
    def load_and_validate_data(self, data_path):
        """Load data with comprehensive validation and cleaning"""
        try:
            self.df = pd.read_csv(data_path)
            print("‚úÖ Data loaded successfully")
            print(f"üìä Dataset shape: {self.df.shape}")
            
            # Data validation and cleaning
            self.clean_data()
            self.validate_experiment_design()
            
        except FileNotFoundError:
            print("‚ùå Error: Data file not found. Please check the path.")
            return None
    
    def clean_data(self):
        """Comprehensive data cleaning with audit trail"""
        print("\nüßπ CLEANING DATA...")
        original_shape = self.df.shape
        
        # Clean group column
        self.df['group'] = self.df['group'].astype(str).str.strip().str.lower()
        
        # Map group names consistently
        group_mapping = {'con': 'control', 'exp': 'treatment', 'experiment': 'treatment'}
        self.df['group'] = self.df['group'].map(group_mapping).fillna(self.df['group'])
        
        # Filter valid groups only
        valid_groups = ['control', 'treatment']
        self.df = self.df[self.df['group'].isin(valid_groups)]
        
        # Standardize column names
        column_mapping = {
            'click': 'converted',
            'session_time': 'session_duration',
            'group': 'variant'
        }
        self.df.rename(columns=column_mapping, inplace=True)
        
        # Handle outliers in session duration (remove extreme values)
        Q1 = self.df['session_duration'].quantile(0.25)
        Q3 = self.df['session_duration'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers_before = len(self.df)
        self.df = self.df[
            (self.df['session_duration'] >= lower_bound) & 
            (self.df['session_duration'] <= upper_bound)
        ]
        outliers_removed = outliers_before - len(self.df)
        
        print(f"   ‚Ä¢ Original shape: {original_shape}")
        print(f"   ‚Ä¢ Final shape: {self.df.shape}")
        print(f"   ‚Ä¢ Outliers removed: {outliers_removed}")
        print(f"   ‚Ä¢ Data quality: {(1 - self.df.isnull().sum().sum() / self.df.size) * 100:.1f}%")
    
    def validate_experiment_design(self):
        """Validate experimental design assumptions"""
        print("\nüîç EXPERIMENT DESIGN VALIDATION")
        
        # Check group balance
        group_counts = self.df['variant'].value_counts()
        total_users = len(self.df)
        
        print("Group Distribution:")
        for group, count in group_counts.items():
            percentage = (count / total_users) * 100
            print(f"   ‚Ä¢ {group.capitalize()}: {count:,} users ({percentage:.1f}%)")
        
        # Statistical test for group balance
        expected_ratio = 0.5
        control_ratio = group_counts['control'] / total_users
        
        if abs(control_ratio - expected_ratio) > 0.05:  # More than 5% deviation
            print("‚ö†Ô∏è  WARNING: Unbalanced group allocation detected")
        else:
            print("‚úÖ Group allocation is balanced")
        
        # Sample size adequacy check
        min_sample_size = 1000  # Rule of thumb
        if total_users < min_sample_size:
            print(f"‚ö†Ô∏è  WARNING: Sample size ({total_users}) may be insufficient")
        else:
            print(f"‚úÖ Sample size adequate ({total_users:,} users)")
    
    def descriptive_analysis(self):
        """Comprehensive descriptive statistics"""
        print("\nüìà DESCRIPTIVE ANALYSIS")
        
        # Overall metrics
        overall_conversion = self.df['converted'].mean()
        avg_session_duration = self.df['session_duration'].mean()
        
        print(f"Overall Conversion Rate: {overall_conversion:.3f} ({overall_conversion*100:.1f}%)")
        print(f"Average Session Duration: {avg_session_duration:.1f} seconds")
        
        # Group-wise analysis
        group_stats = self.df.groupby('variant').agg({
            'converted': ['count', 'sum', 'mean'],
            'session_duration': ['mean', 'median', 'std']
        }).round(3)
        
        print("\nüìä Group-wise Statistics:")
        print(group_stats)
        
        return group_stats
    
    def statistical_tests(self):
        """Comprehensive statistical testing suite"""
        print("\nüß™ STATISTICAL TESTING")
        
        # Separate data by groups
        control_data = self.df[self.df['variant'] == 'control']
        treatment_data = self.df[self.df['variant'] == 'treatment']
        
        results = {}
        
        # 1. Conversion Rate Test (Chi-square test)
        print("1Ô∏è‚É£ CONVERSION RATE ANALYSIS")
        
        control_conversions = control_data['converted'].sum()
        control_total = len(control_data)
        treatment_conversions = treatment_data['converted'].sum()
        treatment_total = len(treatment_data)
        
        # Create contingency table
        contingency_table = np.array([
            [control_conversions, control_total - control_conversions],
            [treatment_conversions, treatment_total - treatment_conversions]
        ])
        
        # Chi-square test
        chi2, p_value_conv, dof, expected = chi2_contingency(contingency_table)
        
        # Effect size (Relative uplift)
        control_rate = control_conversions / control_total
        treatment_rate = treatment_conversions / treatment_total
        relative_uplift = (treatment_rate - control_rate) / control_rate * 100
        
        results['conversion'] = {
            'control_rate': control_rate,
            'treatment_rate': treatment_rate,
            'relative_uplift': relative_uplift,
            'p_value': p_value_conv,
            'chi2_statistic': chi2,
            'significant': p_value_conv < 0.05
        }
        
        print(f"   Control Rate: {control_rate:.4f} ({control_rate*100:.2f}%)")
        print(f"   Treatment Rate: {treatment_rate:.4f} ({treatment_rate*100:.2f}%)")
        print(f"   Relative Uplift: {relative_uplift:+.2f}%")
        print(f"   P-value: {p_value_conv:.6f}")
        print(f"   Result: {'üü¢ SIGNIFICANT' if p_value_conv < 0.05 else 'üî¥ NOT SIGNIFICANT'}")
        
        # 2. Session Duration Test
        print("\n2Ô∏è‚É£ SESSION DURATION ANALYSIS")
        
        # Check normality
        control_sessions = control_data['session_duration']
        treatment_sessions = treatment_data['session_duration']
        
        # Shapiro-Wilk test for normality (sample if too large)
        if len(control_sessions) > 5000:
            control_sample = control_sessions.sample(5000)
            treatment_sample = treatment_sessions.sample(5000)
        else:
            control_sample = control_sessions
            treatment_sample = treatment_sessions
        
        _, p_normal_control = stats.shapiro(control_sample)
        _, p_normal_treatment = stats.shapiro(treatment_sample)
        
        # Choose appropriate test based on normality
        if p_normal_control > 0.05 and p_normal_treatment > 0.05:
            # Use t-test for normal data
            t_stat, p_value_duration = ttest_ind(control_sessions, treatment_sessions)
            test_used = "T-test"
        else:
            # Use Mann-Whitney U test for non-normal data
            u_stat, p_value_duration = mannwhitneyu(control_sessions, treatment_sessions, alternative='two-sided')
            test_used = "Mann-Whitney U test"
        
        # Effect size (Cohen's d for practical significance)
        pooled_std = np.sqrt(((len(control_sessions) - 1) * control_sessions.std()**2 + 
                             (len(treatment_sessions) - 1) * treatment_sessions.std()**2) / 
                            (len(control_sessions) + len(treatment_sessions) - 2))
        cohens_d = (treatment_sessions.mean() - control_sessions.mean()) / pooled_std
        
        results['session_duration'] = {
            'control_mean': control_sessions.mean(),
            'treatment_mean': treatment_sessions.mean(),
            'difference': treatment_sessions.mean() - control_sessions.mean(),
            'cohens_d': cohens_d,
            'p_value': p_value_duration,
            'test_used': test_used,
            'significant': p_value_duration < 0.05
        }
        
        print(f"   Control Mean: {control_sessions.mean():.2f} seconds")
        print(f"   Treatment Mean: {treatment_sessions.mean():.2f} seconds")
        print(f"   Difference: {treatment_sessions.mean() - control_sessions.mean():+.2f} seconds")
        print(f"   Cohen's d: {cohens_d:.3f} ({'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'} effect)")
        print(f"   P-value: {p_value_duration:.6f} ({test_used})")
        print(f"   Result: {'üü¢ SIGNIFICANT' if p_value_duration < 0.05 else 'üî¥ NOT SIGNIFICANT'}")
        
        self.results = results
        return results
    
    def power_analysis(self):
        """Statistical power analysis and sample size recommendations"""
        print("\n‚ö° POWER ANALYSIS")
        
        # Calculate achieved power for conversion rate test
        control_rate = self.results['conversion']['control_rate']
        treatment_rate = self.results['conversion']['treatment_rate']
        
        # Effect size calculation
        p_pooled = (self.df['converted'].sum()) / len(self.df)
        effect_size = abs(treatment_rate - control_rate) / np.sqrt(p_pooled * (1 - p_pooled))
        
        # Sample sizes
        n_control = len(self.df[self.df['variant'] == 'control'])
        n_treatment = len(self.df[self.df['variant'] == 'treatment'])
        
        print(f"   Effect Size: {effect_size:.4f}")
        print(f"   Sample Sizes: Control={n_control:,}, Treatment={n_treatment:,}")
        
        # Confidence intervals for conversion rates
        control_ci = self._proportion_ci(
            self.results['conversion']['control_rate'], n_control
        )
        treatment_ci = self._proportion_ci(
            self.results['conversion']['treatment_rate'], n_treatment
        )
        
        print(f"   Control 95% CI: [{control_ci[0]:.4f}, {control_ci[1]:.4f}]")
        print(f"   Treatment 95% CI: [{treatment_ci[0]:.4f}, {treatment_ci[1]:.4f}]")
    
    def _proportion_ci(self, p, n, confidence=0.95):
        """Calculate confidence interval for proportion"""
        z = stats.norm.ppf((1 + confidence) / 2)
        margin = z * np.sqrt((p * (1 - p)) / n)
        return (max(0, p - margin), min(1, p + margin))
    
    def create_visualizations(self):
        """Create professional publication-ready visualizations"""
        print("\nüé® CREATING VISUALIZATIONS...")
        
        # Create output directory
        os.makedirs("output/charts", exist_ok=True)
        os.makedirs("output/reports", exist_ok=True)
        
        # Set up the plotting style
        plt.rcParams.update({
            'figure.figsize': (12, 8),
            'font.size': 12,
            'axes.titlesize': 16,
            'axes.labelsize': 14,
            'xtick.labelsize': 12,
            'ytick.labelsize': 12,
            'legend.fontsize': 12
        })
        
        # 1. Executive Summary Dashboard
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('A/B Test Executive Dashboard', fontsize=20, fontweight='bold')
        
        # Conversion Rate Comparison
        conv_data = self.df.groupby('variant')['converted'].agg(['count', 'sum', 'mean']).reset_index()
        bars = ax1.bar(conv_data['variant'], conv_data['mean'], 
                      color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black')
        ax1.set_title('Conversion Rate by Variant', fontweight='bold')
        ax1.set_ylabel('Conversion Rate')
        ax1.set_ylim(0, max(conv_data['mean']) * 1.2)
        
        # Add value labels on bars
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{height:.3f}\n({height*100:.1f}%)', 
                    ha='center', va='bottom', fontweight='bold')
        
        # Session Duration Distribution
        sns.boxplot(data=self.df, x='variant', y='session_duration', ax=ax2, palette='Set2')
        ax2.set_title('Session Duration Distribution', fontweight='bold')
        ax2.set_ylabel('Session Duration (seconds)')
        
        # Sample Size Comparison
        sample_sizes = self.df['variant'].value_counts()
        wedges, texts, autotexts = ax3.pie(sample_sizes.values, labels=sample_sizes.index, 
                                          autopct='%1.1f%%', colors=['#FF6B6B', '#4ECDC4'])
        ax3.set_title('Sample Size Distribution', fontweight='bold')
        
        # Statistical Significance Results
        metrics = ['Conversion Rate', 'Session Duration']
        p_values = [self.results['conversion']['p_value'], 
                   self.results['session_duration']['p_value']]
        colors = ['green' if p < 0.05 else 'red' for p in p_values]
        
        bars = ax4.bar(metrics, [-np.log10(p) for p in p_values], color=colors, alpha=0.7)
        ax4.axhline(y=-np.log10(0.05), color='red', linestyle='--', label='Significance Threshold')
        ax4.set_title('Statistical Significance Results', fontweight='bold')
        ax4.set_ylabel('-log‚ÇÅ‚ÇÄ(p-value)')
        ax4.legend()
        
        # Add significance annotations
        for i, (bar, p) in enumerate(zip(bars, p_values)):
            height = bar.get_height()
            significance = '‚úì Significant' if p < 0.05 else '‚úó Not Significant'
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{significance}\np={p:.4f}', 
                    ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.savefig('output/charts/executive_dashboard.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Detailed Statistical Analysis
        self._create_detailed_plots()
        
        print("‚úÖ Visualizations saved in output/charts/")
    
    def _create_detailed_plots(self):
        """Create detailed statistical analysis plots"""
        
        # Conversion Rate with Confidence Intervals
        fig, ax = plt.subplots(figsize=(10, 6))
        
        groups = ['Control', 'Treatment']
        rates = [self.results['conversion']['control_rate'], 
                self.results['conversion']['treatment_rate']]
        
        # Calculate confidence intervals
        control_n = len(self.df[self.df['variant'] == 'control'])
        treatment_n = len(self.df[self.df['variant'] == 'treatment'])
        
        control_ci = self._proportion_ci(rates[0], control_n)
        treatment_ci = self._proportion_ci(rates[1], treatment_n)
        
        errors = [[rates[0] - control_ci[0], rates[1] - treatment_ci[0]], 
                 [control_ci[1] - rates[0], treatment_ci[1] - rates[1]]]
        
        bars = ax.bar(groups, rates, yerr=errors, capsize=10, 
                     color=['#FF6B6B', '#4ECDC4'], alpha=0.8, 
                     edgecolor='black', error_kw={'linewidth': 2})
        
        ax.set_title('Conversion Rates with 95% Confidence Intervals', fontsize=16, fontweight='bold')
        ax.set_ylabel('Conversion Rate')
        ax.set_ylim(0, max(rates) * 1.3)
        
        # Add statistical significance annotation
        uplift = self.results['conversion']['relative_uplift']
        p_value = self.results['conversion']['p_value']
        significance = "Significant" if p_value < 0.05 else "Not Significant"
        
        ax.text(0.5, max(rates) * 1.2, 
               f'Relative Uplift: {uplift:+.1f}%\n{significance} (p={p_value:.4f})', 
               ha='center', fontsize=12, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        
        plt.tight_layout()
        plt.savefig('output/charts/conversion_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_report(self):
        """Generate comprehensive business report"""
        print("\nüìÑ GENERATING BUSINESS REPORT...")
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# A/B Testing Analysis Report
**Generated:** {timestamp}

## Executive Summary

This analysis evaluates the performance of a web user engagement A/B test comparing a control variant against a treatment variant. The test measured two primary metrics: **conversion rate** and **session duration**.

### Key Findings

**Conversion Rate Performance:**
- Control Group: {self.results['conversion']['control_rate']:.4f} ({self.results['conversion']['control_rate']*100:.2f}%)
- Treatment Group: {self.results['conversion']['treatment_rate']:.4f} ({self.results['conversion']['treatment_rate']*100:.2f}%)
- **Relative Uplift: {self.results['conversion']['relative_uplift']:+.2f}%**
- Statistical Significance: {'üü¢ YES' if self.results['conversion']['significant'] else 'üî¥ NO'} (p = {self.results['conversion']['p_value']:.6f})

**Session Duration Performance:**
- Control Group: {self.results['session_duration']['control_mean']:.2f} seconds
- Treatment Group: {self.results['session_duration']['treatment_mean']:.2f} seconds
- **Difference: {self.results['session_duration']['difference']:+.2f} seconds**
- Effect Size (Cohen's d): {self.results['session_duration']['cohens_d']:.3f}
- Statistical Significance: {'üü¢ YES' if self.results['session_duration']['significant'] else 'üî¥ NO'} (p = {self.results['session_duration']['p_value']:.6f})

## Business Recommendations

"""
        
        # Add recommendations based on results
        if self.results['conversion']['significant']:
            if self.results['conversion']['relative_uplift'] > 0:
                report += """
### ‚úÖ RECOMMENDATION: IMPLEMENT TREATMENT VARIANT

The treatment variant shows a statistically significant improvement in conversion rate with a {:.1f}% relative uplift. This improvement is both statistically and practically significant.

**Expected Impact:**
- Implementing this change could increase conversions by approximately {:.1f}%
- Based on current traffic, this translates to meaningful business value

""".format(abs(self.results['conversion']['relative_uplift']), abs(self.results['conversion']['relative_uplift']))
            else:
                report += """
### ‚ùå RECOMMENDATION: MAINTAIN CONTROL VARIANT

The treatment variant shows a statistically significant decrease in conversion rate. The control variant performs better and should be maintained.
"""
        else:
            report += """
### ‚ö†Ô∏è RECOMMENDATION: INCONCLUSIVE - CONSIDER EXTENDED TESTING

The test results do not show statistical significance. Consider:
1. Running the test for a longer period to increase sample size
2. Testing a more dramatic change to increase effect size
3. Segmenting the analysis to identify specific user groups where the treatment performs better
"""
        
        report += f"""
## Statistical Methodology

### Test Setup
- **Sample Size:** {len(self.df):,} users
- **Control Group:** {len(self.df[self.df['variant'] == 'control']):,} users
- **Treatment Group:** {len(self.df[self.df['variant'] == 'treatment']):,} users
- **Test Duration:** [Based on data provided]

### Statistical Tests Applied
1. **Chi-square test** for conversion rate comparison
2. **{self.results['session_duration']['test_used']}** for session duration comparison
3. **Effect size calculations** (Cohen's d) for practical significance
4. **95% Confidence intervals** for precision estimates

### Assumptions Validated
- Random assignment to groups
- Sufficient sample size for statistical power
- Independent observations
- Appropriate statistical tests based on data distribution

## Risk Considerations

- **Multiple Testing:** If this is part of a larger testing program, consider Bonferroni correction
- **Seasonality:** Ensure test period represents typical user behavior
- **Technical Implementation:** Verify proper randomization and tracking

## Next Steps

1. **If implementing:** Plan gradual rollout with monitoring
2. **If not implementing:** Document learnings and plan follow-up tests
3. **Monitoring:** Continue tracking key metrics post-implementation

---
*This report was generated using professional A/B testing statistical analysis methodologies.*
"""
        
        # Save report
        with open('output/reports/ab_test_report.md', 'w') as f:
            f.write(report)
        
        print("‚úÖ Report saved as output/reports/ab_test_report.md")
        
        return report
    
    def run_complete_analysis(self):
        """Execute the complete analysis pipeline"""
        print("üöÄ STARTING COMPREHENSIVE A/B TEST ANALYSIS")
        print("=" * 60)
        
        if self.df is None:
            print("‚ùå Cannot proceed without data")
            return
        
        # Execute analysis pipeline
        self.descriptive_analysis()
        self.statistical_tests()
        self.power_analysis()
        self.create_visualizations()
        self.generate_report()
        
        print("\n" + "=" * 60)
        print("üéâ ANALYSIS COMPLETE!")
        print("\nOutput files generated:")
        print("   üìä Charts: output/charts/")
        print("   üìÑ Report: output/reports/ab_test_report.md")
        print("\nüí° This analysis demonstrates:")
        print("   ‚Ä¢ Statistical rigor and hypothesis testing")
        print("   ‚Ä¢ Professional data visualization")
        print("   ‚Ä¢ Business-focused recommendations")
        print("   ‚Ä¢ Comprehensive reporting")

# Main execution
if __name__ == "__main__":
    # Initialize and run analysis
    analyzer = ABTestAnalyzer("data/ab_test_web_data.csv")
    analyzer.run_complete_analysis()